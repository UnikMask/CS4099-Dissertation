#+TITLE: Determinization and Minimization of Non-Deterministic Finite State Automatas - A Distributed Approach
#+AUTHOR: A. Guerville



#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage[a4paper, total={6in, 8in}]{geometry}
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{automata, positioning, arrow}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \fancyhf{}
#+LATEX_HEADER: \lhead{Determinization and Minimization of Non-Deterministic NFAs - A Distributed Approach}
#+LATEX_HEADER: \fancyfoot[RO, LE] {Page \thepage}


* Introduction
Finite State Automata are one of the most fundamental concepts in Computer Science. Their uses range everywhere from parsing
to mathematical theory. Finite State Automata exist in two kinds: Deterministic Finite Automata (DFAs) and Non-Deterministic
Finite Automata (NFAs).

It is well-known that both structures describe the same set of languages (regular languages), but
it is in general significantly easier to work with DFAs than NFAs. Some problems are first transcribed into NFAs, therefore
the determinization of a DFA into a NFA, and it's minimization, are critical steps into the understanding of those problems.
However, both those problems are computationally complex and memory intensive,
hence the need to do it efficiently, and to distribute the computation towards multiple systems.

Here, a set of determinization and minimization algorithms, ranging from single-threaded to distributed, is described and
solutions are implemented to solve that task, in a sequential, multi-threaded, and multi-process manner.
The algorithms are then tested and compared on a problem class about the determinization of NFAs: Transportation Graphs AKA
Token Passing Networks.

* Related Works
Single-threaded NFA determinization and minization algorithms have existed since the 1950s. DFA determinization's /Rabin-Scott superset construction/ algorithm is a well-known determinization algorithm which has existed for a long time. However, DFA
minimization is younger, and the most well-known minimization algorithm today is Hopcroft's minimization algorithm.

Parallel NFA determinization algorithms have begun being researched round the 1990s. For example, \cite{508056} ran
a parallel NFA determinization and minimization algorithm on a supercomputer, using a message passing model instead of
shared memory.

In 2007, \cite{DBLP_journal} implements a disk-based distributed algorithm for large NFAs. A disk-based approach avoids the
RAM memory space issues from previous implementations.

Later, \cite{Slavici2012AnEP} proposes a general programming model to migrate RAM-based legacy algorithms into parallel
disks - and applies the model to NFA determinization and minimization.

In 2020, \cite{Ba2020OnTD} uses Bulk Synchronous Parallel abstract computer model to implement a more
performant distributed NFA determinization and minization algorithm.

Finally, \cite{A2022ACS} compares both the MapReduce and BSP-based NFA determinization and minimization algorithm, finding
that the BSP/Pregel based solution outperforms the MapReduce solution.


* Token Passing Networks
<<para:tpn>>
A token passing network is a directed graph \(G = (V, E)\) such that:
+ \(V\): Vertices/nodes,
+ \(E \in (V \times label \times V)\): edges - an edge connects a vertex to another, and may contain a label.
+ There exists a single input node \(I\) in \(V\) such that there is not ingoing edges to it -
  \[!\exists I \in V. \nexists v_{2}. \exists v_{1}. \exists e = (v_{1}, v_{2}) \in E. v_{2} = I\]
+ There exists a single output node \(O\) in \(V\) such that there is no ougoing edges from it -
  \[!\exists O \in V. \nexists v_{1}. \exists v_{2}. \exists e = (v_{1}, v_{2}) \in E. v_{1} = O\]

Token Passing Networks, originally called Transportation Graphs by \cite{ATKINSON1997103}, were originally studied
by \cite{ATKINSON1997103} in order to think about what kind of packet permutations might arise from packet delay in networks.

Design patterns in transportation graphs can introduce properties for a transition graph, as well. For example, figure
\ref{fig:infstack} shows the design of an infinite stack data structure, where \(S\) represents an infinite number
of nodes connected as shows figure \ref{fig:stackinsides}.

#+CAPTION: Example of a stack TPN
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture}[main/.style = {draw, circle}]
    \node[main] (1) {$I$};
    \node[main] (2) [right of=1] {$S$};
    \node[main] (3) [right of=2] {$O$};
    \draw[->] (1) -- (2);
    \draw[->] (2) -- (3);
\end{tikzpicture}
\label{fig:infstack}
\end{figure}


#+CAPTION: Inner Workings of a size 3 TPN stack
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture}[main/.style = {draw, circle}]
    \node[main] (1) {$S_{1}$};
    \node[main] (2) [below of=1] {$S_{2}$};
    \node[main] (3) [below of=2] {$S_{3}$};
    \draw[->] (1) -- (2);
    \draw[->] (2) -- (1);
    \draw[->] (2) -- (3);
    \draw[->] (3) -- (2);
\end{tikzpicture}
\label{fig:stackinsides}
\end{figure}

Transportation graphs are used as such:
+ Each node can store one "token",
+ "Tokens" can be fetched from the input node \(I\) to the next node,
+ "Tokens" must be transported to the output node \(O\),
+ After all "tokens" from the input stream are consumed, there should be no tokens remaining in the graph.

Tokens are kept in track by keeping the order at which the tokens arrived in. Therefore, it is possible to study the
possible order at which the tokens arrive at with a given transportation graph.


** Permutation Classes and 3-1-2 avoidance
As described previously, it is possible to describe what possible orders the tokens may arrive at from a token passing
network. Such area of study comes from a property of stacks, which is what kinds of permutations are Stack Sortable.

For example, \cite{Waton2007onPC} describes properties of stacks in regards to what permutations they accept.

#+CAPTION: Graphical Example of a stack
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {Input};
    \node (output) at (0.5, 0.25) {Output};
\end{tikzpicture}
\label{fig:graphstack}
\end{figure}

Figure \ref{fig:graphstack} shows a graphical representation of a stack, with an input stream on the right, containing
a stream of tokens, and an output stream on the left, which accepts tokens. On one hand, figure \ref{fig:graphperms}
describes a permutation which is accepted by a stack.


#+CAPTION: Successful Stack Permutation
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {1 2 3};
    \node (output) at (0, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {2 3};
    \node (s1) at (0.75, -0.75) {1};
    \node (output) at (0, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {3};
    \node (s1) at (0.75, -0.75) {1};
    \node (s2) at (0.75, -0.25) {2};
    \node (output) at (0, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {};
    \node (s1) at (0.75, -0.75) {1};
    \node (s2) at (0.75, -0.25) {2};
    \node (output) at (0, 0.25) {3};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {};
    \node (s1) at (0.75, -0.75) {1};
    \node (output) at (0, 0.25) {3 2};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.25, 0.25) {};
    \node (output) at (0, 0.25) {3 2 1};
\end{tikzpicture}
\label{fig:graphperms}
\end{figure}

On the other hand, \ref{fig:312avoidance} presents a permutation which is is not accepted by a stack. As the figure shows,
it is possible to pass the 3rd token to the output first, but then it is impossible to pass the first token, as token 2 is
at the front. This class of pattern is called 3-1-2 avoidance/ 2-3-1 avoidance.

The 3-1-2 pattern or 2-3-1 pattern depends
on whether the stack tries reorder a sequence of tokens (2-3-1 exclusion), or it tries to permute an ordered sequence.
It is therefore said that 2-3-1 is the inverse pattern of 3-1-2.

#+CAPTION: 3-1-2 Avoidance in a Stack
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {1 2 3};
    \node (output) at (0.5, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {2 3};
    \node (s1) at (1.5, -1.75) {1};
    \node (output) at (0.5, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {3};
    \node (s1) at (1.5, -1.75) {1};
    \node (s2) at (1.5, -1.25) {2};
    \node (output) at (0.5, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {};
    \node (s1) at (1.5, -1.75) {1};
    \node (s2) at (1.5, -1.25) {2};
    \node (output) at (0.5, 0.25) {3};
\end{tikzpicture}
\label{fig:312avoidance}
\end{figure}

Thus, the stack model can be modelled with transportation graphs using a stack of nodes, hence the study of accepted
permutations for a transportation graph.

** Conversion into NFA
A property of token passing networks, is that they can be converted into NFAs, in which the alphabet represents the
rank encoding of a token, and a state is represented by the order of a token on the initial ordered input stream.

The following definition from \cite{Waton2007onPC} defines a /rank encoding/ -
\begin{quote}
The $rank$ $encoding$ of a permutation is generated by replacing each element by its value relative to those
elements which come after it.
\end{quote}

From the rank encoding it is easy to describe the language of all accepted permutations of a transportation graph, hence
the wish to convert transportation graphs into NFAs, and to determinize and minimize them.


* The =nfdeterminize= System
The solution that is implemented for this research is a program named =nfdeterminize=. =Nfdeterminize= is a command-line
application that takes in a finite state machine, either as a file, or by generating a token passing network. Then, it
can either determinize the given NFA, minimize a DFA, or run both functionalities to convert an NFA into a minimal DFA.

** Features
As an NFA determinization and minimization solution that specialises in dealing with token passing network automatas,
=nfdeterminizes= supports the following features:
+ Loading automatas from file under GAP's printing style for finite state automatas,
+ Generating and loading automatas out of token passing network patterns for direct use by the program,
+ Determinizing and minimizing any given finite state automaton,
+ Choosing between algorithms, sequential, multi-threaded and multi-processing implementations for determinization and
  minimization.
+ Outputting the result either on standard output, or to a file that is under GAP's automata format, such that the automata
  may be loaded onto GAP later on.

** Language
The software for this project is written in Rust. Rust[fn:: https://rust-lang.org] is a performance-focused and memory-safe
programming language with a modern approach to memory management by enforcing memory safety. It is a relatively new language,
having only appeared in 2015, and it's distributed computing support is at the current moment relatively limited. However,
it's ecosystem is maturing fairly quickly over time, and has a wide community.

The main reason why Rust is chosen here over other languages is for it's commitment to performance and memory safety -
it performs as well if not better over certain cases than C or C++, but guarantees memory safety by checking for
inconsistencies and memory lifetime at compile time. Furthermore, Rust also deals with concurrent access issues
at compile time, which takes away a lot of strain during development of multi-threaded or multi-process algorithms.

Some other potential languages for implementation are C++, Java or Go.

C++ is a high-performance language as well, with
a mature ecosystem, but does not deal with memory as well as Rust does. Furthermore, Rust, being a more modern language,
has in general more efficient built-in implementations of data structures, such as Rust's =HashMap=, implemented using
the swiss table [fn:: https://abseil.io/about/design/swisstables].

Go is another programming language with performance and memory safety in mind. The main advantage of Go is how it easily
handles concurrency by using goroutines. Furthermore, Go has very solid support for the Docker API and Kubernetes, making it
a good choice for multi-threaded or distributed workloads. However, Go uses a garbage collector during runtime, and in general
performs considerably slower for same programs than Rust or C++ would.

Finally, Java is a high-level, object oriented programming language. Java is the only language in this list that is not
strictly a compiled language, compiling to bytecode that is run on the JVM, instead of running directly on the machine.
Java is not a good choice for this application, as using Java and the JVM comes at severe performance costs.

Overall, Rust was chosen for =nfdeterminize= for the following factors:
+ No sacrifice between memory safety and performance,
+ Good multithreaded suppport thanks to compile time concurrent access checking, shared memory and message-based concurrency
  support,
+ Modern and easy dependency management with =cargo=[fn:: https://doc.rust-lang.org/cargo],


** Distributed Computing in Rust
Rust being a relatively new language, it has at the current moment limited support for Docker. Some small projects exist
however, that wrap the docker API for Rust.

Aside from Docker, Rust processes can still communicate between each other using
either =ipc-channel=[fn:: https://github.com/ip-channel], or using ZeroMQ[fn:: https://zeromq.org]. Therefore, it is possible
to make distributed systems running Rust, but managing the network cluster or processes may be more easily done using
another language such as Python. Nevertheless, Rust is a nice solution to implement the performance-critical parts of a
distributed system.

In the context of =nfdeterminize=, a distributed system is simulated by spawning processes on the same machine, and having
them communicate using ZeroMQ. While =ipc-channel= passes file descriptors over sockets under Unix for inter-process messaging,
ZeroMQ is more complete, as it supports a lot more kinds of transports such as TCP, which =nfdeterminize= uses.

* Sequential Approach
** Approach to Determinization
First of all, NFA determinization is a well-known process, and efficient algorithms for it have existed for a long time.
The most widely-used algorithm for determinization is the superset construction algorithm, which explores the NFA from node
to node, keeping track of the sets of states visited in a map, until we've explored all reachable nodes.

The major advantage of this algorithm over any other is that it only explores reachable states in the NFA, and produces only
reachable states in the resulting DFA. The consequences are two-fold:
+ 1. The amount of exploration involved is severely decreased, depending on the NFA that is determinized,
+ 2. There is no need to remove unreachable states from the resulting DFA after determinization and before minimization.

The algorithm possesses shared memory in form of \(M\), the structure that maps a kept set of states to the number that it
is assigned on the final DFA, because the algorithm needs to check if a state has already been found after producing it.

#+CAPTION: Rabin Scott's Superset Construction Algorithm
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{SupersetConstruction}{\(M = (S, \Sigma, \delta, S_{0}, T)\)}
\State \(M\) \gets [(\(S_{0}, 0\))]
\State \(T^{'}\) \gets []
\If{\(\exists s \in S_{0}. s \in T\)}
    \State \(T^{'}\) \gets [\(S_{0}\)]
\EndIf
\State \(F\) \gets [\(S_{0}\)]
\While{\(F \neq \emptyset\)}
    \State \(S_{next}\) \gets pop from \(F\)
    \ForAll{\(a \in \Sigma\)}
        \State \(S^{'}\) \gets {}
        \ForAll{\(s \in S_{next}\)}
            \State Add \(s\) and all \epsilon transitions from \(s\)to \(S^{'}\)
        \EndFor
        \If{\(S_{next} \notin M\)}
            \State \(M\) \gets [\(M\), \((S^{'}, \left| M \right|)\)]
            \If{\(\exists s \in T. s \in S^{'}\)}
                \State \(T^{'}\) \gets [\(T^{'}, S^{'}\)]
            \EndIf
            \State \(F\) \gets [\(F\), \(S^{'}\)]
        \EndIf
        \State \(\delta^{'}\) \gets [\(\delta^{'}\), (\(S_{next}\), \(a\), \(S^{'}\))]
    \EndFor
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

Complexity-wise, the worst-case time complexity of the superset construction is \(O(2^{n})\), where \(n\) is the number of states in the original NFA.
Such worst-case is unavoidable as the size of the superset of states in the NFA \(\left| \mathds{S}(S) \right| = 2^{\left| S \right|}\), where \(S\) is the set of states in the original NFA.
However, this treshold is generally never reached, hence the purpose of the superset construction algorithm.

In terms of implementing the sequential version of the superset construction algorithm, most of the design decision comes in
how to store sets of states, as a state should be able to describe one of \(2^{n}\) possible states.

*** Storing Sets of States
The main challenge of superset construction implementation is not the implementation of the exploration algorithm, but
rather how to represent states of superset construction. The issues stems from how in superset construction, there
are about \(2^{n}\) possibly reachable states, so it is required to find a fast and memory-efficient way to store
such a state in a hash map. Furthermore, \cite{DBLP_journal} states that in a 2 billion-state DFA, each DFA state may
consist of upto 20 of the NFA states. Therefore, it is definitely required shorten the size of a state.

The solution implemented in the program is as such -
+ During superset construction, when a new state is being searched, represent the set of states as an array of bits.
  This representation is useful as bitwise operations can be done upon it, for a low cost.

+ Then, before hashing the set and inserting it to a hash map, compress the array. Here, the lz4 algorithm is used.
  The lz4 algorithm is a modern and fast byte array compression method that may simply return a byte array. It's main
  advantage is its speed compared to that of other compression algorithms, although it is not as size efficient.

+ The compressed array is inserted into the hash map. State storage size has been decreased for a moderate speed cost.

In =nfdeterminize=, this data structure is defined as a =Ubig= struct, which stands for =unsigned integer=. It is defined in
the =ubig.rs= source file.

*** \epsilon transitions

It is important to note during determinization, that the majority of a token passing network's automaton's transitions are
\epsilon transitions. However, \epsilon transitions in general contribute lightly to the final DFA, and take up a considerable amount
of time when constructing a set of states during superset construction. There are 2 solutions to this problem:
+ Caching the final set of states for all states in the NFA, and lookup the cache when the state is checked again later on,
+ Removing \epsilon transitions from the NFA before feeding it to the determinization algorithm.

However, these approaches present some severe downsides.

Firstly, caching takes uses extra memory for the problem, which
may be better spent for supporting bigger automatas. Counterintuitively, this approach may also become slower for some
NFAs as the time taken adding a set of states to the cache has to be taken into account. The =--cache= argument on
the =run= and =determinize= commands add caching to the determinization process.

On the other hand, building a new NFA without \epsilon transitions is a sort of transformation that can actually increase the
size of the NFA that will be determinized. The disadvantages here are two-fold - first, extra time is taken making
an new automaton, which may not be necessary for some NFAs. Second, increasing the size of the NFA may impact
the time taken by determinization. Therefore, in many cases, \epsilon transition free NFA construction may be slower than
running NFA determinization with \epsilon transitions taken into account. Nevertheless, the feature is still supported using
the =--no-epsilon= argument on the =run= and =determinize= commands.

Overall, the performance of caching and \epsilon transition free automata construction depend heavily on the kind of automata that
is loaded to the program. In the context of token passing networks, it is preferable to generate automatas from specific
patterns like two-stack or buffer-and-stack, for which running determinization directly is preferred.

** Approach to Minimization
While NFA determinization has been a well-known subject for a long time, DFA minimization on the other hand has less well-known algorithms. Out of all the minimization algorithms nowadays, 2 stand out as better
algorithms than the rest. Those are Hopcroft's algorithm and Brzozowski's minimization algorithm.

*** Hopcroft's Algorithm
<<para:hopcroft>>

Hopcroft's algorithm, made by J. Hopcroft in 1971\cite{Hopcroft1971AnNL},  is the first, and probably
the most well-known non-\(O(n^{2})\) time complexity DFA minimization algorithm. It is one of the first partition refinement algorithms.

Hopcrof't algorithm separates the states of the DFA into a partition of 2 sets - accept states and non-accept states. Those will be the states of the minimal automata by the end of the algorithm's execution.
Then, until the frontier is empty, it searches for states in the partition for which the transitions lead to distinguishable states.


If it is the case, then it means the partition has to be divided further. The algorithm is repeated until all states in each partition contain states that are indistinguishable by their transitions,
which means that the resulting DFA holds the same language than the original one, but at it's minimal size.

For definitions, let:
+ \(\mathcal{P}\): the partition to refine,
+ \(P \in \mathcal{P}\): a set of states in the partition.

Hopcroft's algorithm relies on the following lemma -
\newtheorem{lemma}{Lemma}
\begin{lemma}

Let some finite state machine \(M = (S, \Sigma, \delta, S_{p}, T)\).

\(\forall p \in S. \forall q \in S. \forall a \in \Sigma\), let \(\delta(p, a) = p^{'}\), \(\delta(q, a) = q^{'}\).

\(p^{'}\) and \(q^{'}\) are distinguishable \(\Rightarrow\) \(p\) and \(q\) are distinguishable.

\end{lemma}

Therefore, Hopcroft's algorithm uses the reverse transitions of the next set in the frontier to establish distinguishability between states in a set of the partition. Distinguishibility is therefore
defined as such, for some sets \(V, P \in \mathcal{P}\), and \(\delta^{-1}(P, a)\) the set of states \(s \in S s.t \delta(s, a) \in P\):

\[V \cap \delta^{-1}(P, a) \neq \emptyset \wedge V \backslash \delta^{-1}(P, a) \neq \emptyset \Rightarrow V \text{ is distinguishible into } V \cap \delta^{-1}(P, a) \text{ and } V \backslash \delta^{-1}(P, a)\]



#+CAPTION: Hopcroft's Algorithm
\begin{algorithm}
\label{algo:hopcroft}
\begin{algorithmic}[1]
\Procedure{HopcroftAlgo}{\(M = (S, \Sigma, \delta, s_{0}, T)\)}
    \State \(\mathcal{P}\) \gets \([T, S \backslash T]\)
    \State \(Q\) \gets \([T, S \backslash T]\)
    \While{\( \left| Q \right| \neq 0\)}
        \State \(P_{next}\) \gets pop \(Q\)
        \ForAll{a \in \(\Sigma\), V \in \(\mathcal{P}\)} \label{algo:hopcroft:forall}
            \If{\(\delta^{-1}(P_{next}, a) \cap V \neq \emptyset \cap V \backslash \delta^{-1}(P_{next}, a) \neq \emptyset\)}
                \State remove \(V\) from \(P\)
                \State push \(\delta^{-1}(P_{next}, a) \cap V\) into \(P\)
                \State push \(V \backslash \delta^{-1}(P_{next}, a)\) into \(P\)
                \If{\(V \in Q\)}
                    \State replace \([V]\) in \(Q\) with \([V \backslash \delta^{-1}(P_{next}, a), \delta^{-1}(P_{next}, a) \cap V]\)
                \ElsIf{\(\left| V \backslash \delta^{-1}(P_{next}, a) \right| \leq \left| \delta^{-1}(P_{next}, a) \cap V \right|\)}
                    \State add \(V \backslash \delta^{-1}(P_{next}, a)\) to \(Q\)
                \Else
                    \State add \(\delta^{-1}(P_{next}, a) \cap V\) to \(Q\)
                \EndIf
            \EndIf
        \EndFor
    \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

Hopcroft's Algorithm, as shown on figure \ref{algo:hopcroft}, has asymptotic time complexity \(O(knlog(n))\)\cite{Hopcroft1971AnNL}, where:
+ \(k\): the number of input letters in the alphabet \(\Sigma\),
+ \(n\): the number of states in the initial DFA.

This time complexity makes it the minimization algorithm that achieves the best possible time complexity.

Implementation-wise, the approach here is closer to the implementation described in \cite{Yingjie2009DescribingAN}, with some performance improvements.
On line \ref{algo:hopcroft:forall} of \ref{algo:hopcroft}, instead of looking for all \(V\) in \(\mathcal{P}\), it is possible to iterate through all partitions
linked to a state in \(\delta^{-1}(P_{next}, a)\), by keeping a map of what state is linked to which set in \(\mathcal{P}\). Doing so avoids the lengthy process of iterating
through \(\mathcal{P}\) for every set \(P_{next}\) in the frontier.

On the rust implementation, sets are represented as ordered vectors. With ordered vectors, difference and intersection construction can be done in \(O(n)\) time complexity, and
ordered vector construction from inverse transformation is done in \(O(n log(n))\) time complexity, for \(n\) the size of the set. Using a vector instead of a set avoids the overhead gotten from
consistently hashing values into a hash set.

Finally, the queue \(Q\) is done in a circular ring buffer as using contiguous memory, instead of a linked list, for faster memory access, while the partition is done as a simple contiguous memory array, as
it is never needed to pop anything from it. Instead, adding to the partition is done by replacing \(V\) by \(V \cap \delta^{-1}(P_{next}, a)\) and appending \(V \backslash \delta^{-1}(P_{next}, a)\) to the end of \(\mathcal{P}\).

*** Brzozowski's Algorithm

Brzozowski's algorithm is an exception to the general landscape of DFA minimization algorithms. Most minimization algorithms
work by doing partition refinement, like Hopcroft's, and some work by fusion like Revuz's \cite{REVUZ1992181}.
However, Brzozowski's algorithm works, for some finite state machine \(M = (S, \Sigma, \delta, S_{0}, T)\), by
determinizing \(M^{R} = (S, \Sigma, \delta^{-1}, T, S_{0})\), where \(\delta^{-1}\) is the table of inverse transitions from \(M\). Then, perform

determinization of \((M^{R})^{R}\). The result of the determinized \((M^{R})^{R}\) is the minimal DFA representation of \(M\).


This algorithm is very easy to implement as determinization has already been implemented beforehand. However, as with determinization,
it has an exponential time complexity.

Performance-wise however, Brzozowski's is known to outperform other minimization algorithm in particular cases, so it is interesting to support. Here, it
is supported via arguments to the =run= and =determinize= commands of =nfdeterminize=.


* Multithreaded Approach
** Towards a Multithreaded Approach
Sequential implementations of NFA determinization and minimization provide excellent results in general for decently-sized
automatas. However, as the size of the given automatas increases, single threaded performance does not suffice, hence the
need to extend the determinization and minimization process to multiple threads, or multiple machines.

The main challenge of rewriting determinization and minimization comes from the amount of shared memory used during
superset construction and Hopcroft's algorithm. While superset construction can be relatively well separated between
threads and processes, it is harder to do so using Hopcroft's algorithm, as it is optimised for sequential use.
** New Algorithms
*** Determinization
To extend the superset construction algorithm for multithreaded usage and avoid as much use of shared memory,
the algorithm is modified as such, for some number \(k\) of threads:
+ Divide the frontier of research into \(k\) frontiers, such that each thread has access to its own queue,
+ Divide the state to number map to \(k\) parts, accessed by each thread. Each thread hashes the set of states it
   constructed to determine which hash map the set of states should go to,
+ The main thread keeps track of which thread's frontier is empty, and sends a signal to each thread to kill itself once
  all frontiers for each thread is empty, which means all the graph has been explored.

This approach mainly uses mutexes for access to shared memory instead of inter-thread messaging. Messaging-based approach
will instead be used later.

The reason the superset construction algorithm works well for a multithreaded approach is because most of the computation
going on during superset construction happens when constructing a new state from the frontier, which is a step that
does not require shared memory. Furthermore, shared memory issues are inhibited by the sharding of the map and the frontier.
Thus, superset construction greatly profits from multithreading and multiprocessing.

#+CAPTION: Multithreaded Superset Construction Algorithm
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{SupersetConstruction}{\(M = (S, \Sigma, \delta, S_{0}, T)\)}
\State \(M\) \gets [(\(S_{0}, 0\))]
\State \(T^{'}\) \gets []
\If{\(\exists s \in S_{0}. s \in T\)}
    \State \(T^{'}\) \gets [\(S_{0}\)]
\EndIf
\State \(F\) \gets [\(S_{0}\)]
\While{\(F \neq \emptyset\)}
    \State \(S_{next}\) \gets pop from \(F\)
    \ForAll{\(a \in \Sigma\)}
        \State \(S^{'}\) \gets {}
        \ForAll{\(s \in S_{next}\)}
            \State Add \(s\) and all \epsilon transitions from \(s\)to \(S^{'}\)
        \EndFor
        \If{\(S_{next} \notin M\)}
            \State \(M\) \gets [\(M\), \((S^{'}, \left| M \right|)\)]
            \If{\(\exists s \in T. s \in S^{'}\)}
                \State \(T^{'}\) \gets [\(T^{'}, S^{'}\)]
            \EndIf
            \State \(F\) \gets [\(F\), \(S^{'}\)]
        \EndIf
        \State \(\delta^{'}\) \gets [\(\delta^{'}\), (\(S_{next}\), \(a\), \(S^{'}\))]
    \EndFor
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

*** Minimization
While NFA determinization is relatively easy to extend to multithreaded variants, NFA minimization is much harder to
parallelize. Hopcroft's minimization algorithm is still the currently best minimization algorithm in terms of time
complexity, as explained in \ref{para:hopcroft}. However, Hopcroft's algorithm is not efficiently parallelizable.
Therefore an alternative approach is needed to implement multithreaded minimization.

The DFA minimization parallelization started being approached around the 1990s. Here, a parrallelizable minimization
algorithm called /partition refinement/ is used, first implemented by \cite{Slavici2012AnEP}, with the exception
that a RAM-based implementation is used in =nfdeterminize='s case.
#+TODO: Pseudocode shared memory minimization algo
** Benchmarking
#+TODO: Results against sequential approach


* Benchmarking

** Behaviour Testing
The first step in the testing and benchmarking process is to test for correctness of the programs' behaviours. Here,
it is done by unit testing the determinization and minimization algorithm by using NFA and DFA examples that rely on
particular behaviours of each of the determinization and minimization algorithms.

*** Testing Determinization
*** Testing Minimization
** Benchmarking
By context of the research, it is natural that most of the test cases used to gauge
performance of the system are token passing networks. To do so, a set of benchmarks is used to
compare the performance of the multiple system configurations on a set of sce

On all cases, benchmarks are run on a 12-threads Intel i5-11400 machine, with 32GB of RAM. All iterations
take 25 run samples, with 3 seconds of warmup between iterations. Such methods avoids skewing the results
down because of continuous CPU operation.

Results are written to a CSV file, which is then used to generate comparison tables based on performance.

*** GAP-generated NFAs
First of all, automatons generated by GAP are used to test determinization and minimization. GAP \cite{GAPSystem} is
a system for computational discrete algebra, which provides a programming language and a couple of libraries, two of
which being =Automata= and =PatternClass=. The =PatternClass= library provides methods to generate multiple kinds of token passing
networks, such as the buffer and stack TPN, and some functions to generally convert graphs into NFAs.

The main property of GAP-generated NFAs is that they generate states in the NFA out of the nodes of the graph and not out of
data structures, which ends up building a lot of \epsilon transitions which, in the end, will get removed during determinization.
The main advantage of using GAP generated NFAs is to stress test how well determinization handles \epsilon transitions when
finding new states.

In regards to benchmarking, buffer-and-stack NFAs are generated using GAP - from buffer size 2 to 3, and stack size 2 to 7.

*** Self-generated NFAs
On top of the NFAs generated by GAP, the program is also able to generate its own NFAs out of token passing network
patterns. While GAP has a general algorithm for converting TPNs into NFAs, which leads to NFAs with lots of extra information
in form of \epsilon transitions, self-generated NFAs are optimised for the patterns they're built for. This means that the leading
NFA has less \epsilon transitions but still describes the same language. Therefore it is preferred to generate NFAs this way
when researching the language of permutations described by a TPN.

In regards to benchmarking, and to keep benchmark speeds fast enough, buffer-and-stack NFAs and two-stack NFAs are used.
In research, buffer-and-stack TPNs are generally studied as simplifications of two-stack TPNs. In practice, both
kinds of TPNs are used to stress test different parts of the system.
+ 3-buffer-and-k-stack TPNs tend to stress test the determinization process more. For quick benchmarking, buffer-and-stack
  TPNs of buffer size 2 to 3, and stack sizes 2 to 7 are used to compare the speeds of different implementations.
+ 3-stack-and-k-stack TPNs tend to stress minimization more as, by observation, they are usually poorly minimizable.
  two-stack TPNS of first stack size 2 to 3, and second stack size 3 to 5 are used.

Finally, to measure the speed of each implementation, a measure of \(k\) for the biggest 3-buffer-\(k\)-stack TPN that
can be determinized and determinized in under a minute.


* Results
** Comparison of Test Cases
** Sequential Algorithm Evaluation
** Multithreading Against Sequential
** Multiprocessing Approach and Scaling

* Conclusion

* Appendix
** Software Tests
The main purpose of NFA determinization and minimization in the context of this paper is the description of the language
accepted by transportation graphs, as described in \ref{para:tpn}. Therefore, testing here is crucial
for experimental correctness. Here, a presentation of the various unit tests done for the software is
shown to prove the correctness of NFA determinization and minimization implementations.
*** Approach
During development, the approach to development of the multiple determinization and minimization
algorithms is done in a Test Driven Approach:
+ The unit tests for determinization and minimization were written first, as a way to test for correct
  algorithm behaviour,
+ Each iteration of the minimization and determinization algorithms are tested upon those sets of tests,
  and considered sound if they pass the test suite.

This development approach allowed to both guarantee that implementations were correct, and
allow some level of incremental development by continuously writing new algorithms and testing them
against the test suite.
*** Determinization
<<appendix:nfatodfa>>

Determinizations of NFAs are DFAs that often show certain patterns. For example, a determinization of an NFA often possesses
a "sinkhole" state for which all transitions coming from it come back to the state. Other behaviours should be clearly
defined, such as how determinization deals with \(\epsilon\) transitions. Therefore, unit tests check that the behaviours that
define determinization are strictly followed, hence proving the correctness of the algorithm.

The figures below list the multiple patterns that were tested during determinization testing.

#+CAPTION: Redundant NFA to redundant DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=3cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}


#+CAPTION: Empty language NFA to empty language DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \draw (s0) edge[above] node{a, b} (s1)
          (s1) edge[loop above] node{a, b} (s1);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}

#+CAPTION: Unreachable state in NFA removed in the DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=1.5cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=3cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}

#+CAPTION: NFA to DFA with sinkhole
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=1.7cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \node[state, accepting, right of=s1] (s2) {\(s_{2}\)};
    \draw (s0) edge[above] node{a} (s1)
        (s1) edge[above] node{a} (s2);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=1.7cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \node[state, below of=s1] (s2) {\(s_{2}\)};
    \node[state, accepting, right of=s1] (s3) {\(s_{3}\)};
    \draw (s0) edge[above] node{a} (s1)
        (s1) edge[above] node{a} (s3)
        (s0) edge[below] node{b} (s2)
        (s1) edge[right] node{b} (s2)
        (s3) edge[below] node{a, b} (s2)
        (s2) edge[loop below] node{a, b} (s2)
\end{tikzpicture}
\label{fig:redundant}
\end{figure}


#+CAPTION: NFA to DFA with sets of NFA states for states
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, accepting, right of=s0] (s1) {\(s_{1}\)};
    \draw (s0) edge[above] node{a} (s1)
        (s0) edge[loop above] node{a} (s0)
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\([s_{0}]\)};
    \node[state, accepting, right of=s0] (s1) {\([s_{0}, s_{1}]\)};
    \draw (s0) edge[above] node{a} (s1)
        (s1) edge[loop above] node{a} (s1)
\end{tikzpicture}
\label{fig:redundant}
\end{figure}



#+CAPTION: \epsilon automaton to DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \node[state, below of=s0] (s2) {\(s_{2}\)};
    \node[state, accepting, below of=s1] (s3) {\(s_{3}\)};
    \draw (s0) edge[above] node{\(\epsilon\)} (s1)
        (s0) edge[left] node{a} (s2)
        (s1) edge[right] node{a} (s3)
        (s2) edge[below] node{b} (s3)
        (s3) edge[loop below] node{a, b} (s3);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, 0) {};
    \draw (0, 2) -> (1, 2);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\([s_{0}]\)};
    \node[state, accepting, right of=s0] (s1) {\([s_{1}, s_{3}]\)};
    \node[state, below of=s0] (s2) {\([]\)};
    \node[state, accepting, below of=s1] (s3) {\([s_{3}]\)};
    \draw (s0) edge[above] node{\(\epsilon\)} (s1)
        (s0) edge[left] node{a} (s2)
        (s1) edge[right] node{a} (s3)
        (s2) edge[loop below] node{a, b} (s2)
        (s3) edge[loop below] node{a, b} (s3);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}

*** Minimization
Minimization is tested similarly to the way determinization is tested, by testing on DFAs that are minimially bipartite,
some with a single separation of sets within a partition, and some unminimizable DFA.

Those tests are not as detailed behaviour-wise as the unit tests for determinization, and are more specialized towards
Hopcroft's algorithm. However, they do demonstrate some level of correctness in the algorithm.

The figures below demonstrate the patterns and cases that were tested to show the implementation
correctness of the written minimization algorithms.

\bibliographystyle{ieeetr}
\bibliography{dissertation}
