#+TITLE: Determinization and Minimization of Non-Deterministic Finite State Automatas - A Distributed Approach
#+AUTHOR: A. Guerville



#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage[a4paper, total={6in, 8in}]{geometry}
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{automata, positioning, arrow}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \fancyhf{}
#+LATEX_HEADER: \lhead{Determinization and Minimization of Non-Deterministic NFAs - A Distributed Approach}
#+LATEX_HEADER: \fancyfoot[RO, LE] {Page \thepage}


* Introduction
Finite State Automata are one of the most fundamental concepts in Computer Science. Their uses range everywhere from parsing
to mathematical theory. Finite State Automata exist in two kinds: Deterministic Finite Automata (DFAs) and Non-Deterministic
Finite Automata (NFAs). It is well-known that both structures describe the same set of languages (regular languages), but
it is in general significantly easier to work with DFAs than NFAs. Some problems are first transcribed into NFAs, therefore
the determinization of a DFA into a NFA, and it's minimization, are critical steps into the understanding of those problems.

Here, a set of determinization and minimization algorithms, ranging from single-threaded to distributed, is described and
implemented to solve that task, and tested on a problem class about the determinization of NFAs: Transportation Graphs AKA
Token Passing Networks.

* Related Works
Single-threaded NFA determinization and minization algorithms have existed since the 1950s. DFA determinization's /Rabin-Scott superset construction/ algorithm is a well-known determinization algorithm which has existed for a long time. However, DFA
minimization is younger, and the most well-known minimization algorithm today is Hopcroft's minimization algorithm.

Parallel NFA determinization algorithms have begun being researched round the 1990s. For example, \cite{508056} ran
a parallel NFA determinization and minimization algorithm on a supercomputer, using a message passing model instead of
shared memory.

In 2007, \cite{DBLP_journal} implements a disk-based distributed algorithm for large NFAs. A disk-based approach avoids the
RAM memory space issues from previous implementations.

Later, \cite{Slavici2012AnEP} proposes a general programming model to migrate RAM-based legacy algorithms into parallel
disks - and applies the model to NFA determinization and minimization.

In 2020, \cite{Ba2020OnTD} uses Bulk Synchronous Parallel abstract computer model to implement a more
performant distributed NFA determinization and minization algorithm.

Finally, \cite{A2022ACS} compares both the MapReduce and BSP-based NFA determinization and minimization algorithm, finding
that the BSP/Pregel based solution outperforms the MapReduce solution.


* Token Passing Networks
<<para:tpn>>
A token passing network is a directed graph \(G = (V, E)\) such that:
+ \(V\): Vertices/nodes,
+ \(E \in (V \times label \times V)\): edges - an edge connects a vertex to another, and may contain a label.
+ There exists a single input node \(I\) in \(V\) such that there is not ingoing edges to it -
  \[!\exists I \in V. \nexists v_{2}. \exists v_{1}. \exists e = (v_{1}, v_{2}) \in E. v_{2} = I\]
+ There exists a single output node \(O\) in \(V\) such that there is no ougoing edges from it -
  \[!\exists O \in V. \nexists v_{1}. \exists v_{2}. \exists e = (v_{1}, v_{2}) \in E. v_{1} = O\]

Token Passing Networks, originally called Transportation Graphs by \cite{ATKINSON1997103}, were originally studied
by \cite{ATKINSON1997103} in order to think about what kind of packet permutations might arise from packet delay in networks.

Design patterns in transportation graphs can introduce properties for a transition graph, as well. For example, figure
\ref{fig:infstack} shows the design of an infinite stack data structure, where \(S\) represents an infinite number
of nodes connected as shows figure \ref{fig:stackinsides}.

#+CAPTION: Example of a stack TPN
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture}[main/.style = {draw, circle}]
    \node[main] (1) {$I$};
    \node[main] (2) [right of=1] {$S$};
    \node[main] (3) [right of=2] {$O$};
    \draw[->] (1) -- (2);
    \draw[->] (2) -- (3);
\end{tikzpicture}
\label{fig:infstack}
\end{figure}


#+CAPTION: Inner Workings of a size 3 TPN stack
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture}[main/.style = {draw, circle}]
    \node[main] (1) {$S_{1}$};
    \node[main] (2) [below of=1] {$S_{2}$};
    \node[main] (3) [below of=2] {$S_{3}$};
    \draw[->] (1) -- (2);
    \draw[->] (2) -- (1);
    \draw[->] (2) -- (3);
    \draw[->] (3) -- (2);
\end{tikzpicture}
\label{fig:stackinsides}
\end{figure}

Transportation graphs are used as such:
+ Each node can store one "token",
+ "Tokens" can be fetched from the input node \(I\) to the next node,
+ "Tokens" must be transported to the output node \(O\),
+ After all "tokens" from the input stream are consumed, there should be no tokens remaining in the graph.

Tokens are kept in track by keeping the order at which the tokens arrived in. Therefore, it is possible to study the
possible order at which the tokens arrive at with a given transportation graph.


** Permutation Classes and 3-1-2 avoidance
As described previously, it is possible to describe what possible orders the tokens may arrive at from a token passing
network. Such area of study comes from a property of stacks, which is what kinds of permutations are Stack Sortable.

For example, \cite{Waton2007onPC} describes properties of stacks in regards to what permutations they accept.

#+CAPTION: Graphical Example of a stack
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {Input};
    \node (output) at (0.5, 0.25) {Output};
\end{tikzpicture}
\label{fig:graphstack}
\end{figure}

Figure \ref{fig:graphstack} shows a graphical representation of a stack, with an input stream on the right, containing
a stream of tokens, and an output stream on the left, which accepts tokens. On one hand, figure \ref{fig:graphperms}
describes a permutation which is accepted by a stack.


#+CAPTION: Successful Stack Permutation
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {1 2 3};
    \node (output) at (0, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {2 3};
    \node (s1) at (0.75, -0.75) {1};
    \node (output) at (0, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {3};
    \node (s1) at (0.75, -0.75) {1};
    \node (s2) at (0.75, -0.25) {2};
    \node (output) at (0, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {};
    \node (s1) at (0.75, -0.75) {1};
    \node (s2) at (0.75, -0.25) {2};
    \node (output) at (0, 0.25) {3};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.5, 0.25) {};
    \node (s1) at (0.75, -0.75) {1};
    \node (output) at (0, 0.25) {3 2};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (-0.25, 0) -- (0.5, 0) -- (0.5, -1) -- (1, -1) -- (1, 0) -- (1.75, 0);
    \node (input) at (1.25, 0.25) {};
    \node (output) at (0, 0.25) {3 2 1};
\end{tikzpicture}
\label{fig:graphperms}
\end{figure}

On the other hand, \ref{fig:312avoidance} presents a permutation which is is not accepted by a stack. As the figure shows,
it is possible to pass the 3rd token to the output first, but then it is impossible to pass the first token, as token 2 is
at the front. This class of pattern is called 3-1-2 avoidance/ 2-3-1 avoidance.

The 3-1-2 pattern or 2-3-1 pattern depends
on whether the stack tries reorder a sequence of tokens (2-3-1 exclusion), or it tries to permute an ordered sequence.
It is therefore said that 2-3-1 is the inverse pattern of 3-1-2.

#+CAPTION: 3-1-2 Avoidance in a Stack
#+ATTR_LATEX: :float nil
\begin{figure}
\centering
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {1 2 3};
    \node (output) at (0.5, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {2 3};
    \node (s1) at (1.5, -1.75) {1};
    \node (output) at (0.5, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {3};
    \node (s1) at (1.5, -1.75) {1};
    \node (s2) at (1.5, -1.25) {2};
    \node (output) at (0.5, 0.25) {};
\end{tikzpicture}
\begin{tikzpicture} [main/.style = {draw}]
    \draw (0, 0) -- (1, 0) -- (1, -2) -- (2, -2) -- (2, 0) -- (3, 0);
    \node (input) at (2.5, 0.25) {};
    \node (s1) at (1.5, -1.75) {1};
    \node (s2) at (1.5, -1.25) {2};
    \node (output) at (0.5, 0.25) {3};
\end{tikzpicture}
\label{fig:312avoidance}
\end{figure}

Thus, the stack model can be modelled with transportation graphs using a stack of nodes, hence the study of accepted
permutations for a transportation graph.

** Conversion into NFA
A property of token passing networks, is that they can be converted into NFAs, in which the alphabet represents the
rank encoding of a token, and a state is represented by the order of a token on the initial ordered input stream.

The following definition from \cite{Waton2007onPC} defines a /rank encoding/ -
\begin{quote}
The $rank$ $encoding$ of a permutation is generated by replacing each element by its value relative to those
elements which come after it.
\end{quote}

From the rank encoding it is easy to describe the language of all accepted permutations of a transportation graph, hence
the wish to convert transportation graphs into NFAs, and to determinize and minimize them.

* Testing & Benchmarking
The main purpose of NFA determinization and minimization in the context of this paper is the description of the language
accepted by transportation graphs, as described in \ref{para:tpn}. Therefore, testing and benchmarking here involves two main
components: testing the correctness of the program's behaviour, and being able to determinize and minimize token passing
networks fast.

** Behaviour Testing
The first step in the testing and benchmarking process is to test for correctness of the programs' behaviours. Here,
it is done by unit testing the determinization and minimization algorithm by using NFA and DFA examples that rely on
particular behaviours of each of the determinization and minimization algorithms.

*** Testing Determinization
Determinizations of NFAs are DFAs that often show certain patterns. For example, a determinization of an NFA often possesses
a "sinkhole" state for which all transitions coming from it come back to the state. Other behaviours should be clearly
defined, such as how determinization deals with \(\epsilon\) transitions. Therefore, unit tests check that the behaviours that
define determinization are strictly followed, hence proving the correctness of the algorithm.

Part \ref{appendix:nfatodfa} of the appendix lists the multiple patterns that were tested during determinization testing.

*** Testing Minimization
Minimization is tested similarly to the way determinization is tested, by testing on DFAs that are minimially bipartite,
some with a single separation of sets within a partition, and some unminimizable DFA.

Those tests are not as detailed behaviour-wise as the unit tests for determinization, and are more specialized towards
Hopcroft's algorithm. However, they do demonstrate some level of correctness in the algorithm.

** Benchmarking
#+TODO: GAP-generated NFAs
#+TODO: Self-generated NFAS

* Sequential Approach
** Approach to Determinization
First of all, NFA determinization is a well-known process, and efficient algorithms for it have existed for a long time.
The most widely-used algorithm for determinization is the superset construction algorithm, which explores the NFA from node
to node, keeping track of the sets of states visited in a map, until we've explored all reachable nodes.

The major advantage of this algorithm over any other is that it only explores reachable states in the NFA, and produces only
reachable states in the resulting DFA. The consequences are two-fold:
+ 1. The amount of exploration involved is severely decreased, depending on the NFA that is determinized,
+ 2. There is no need to remove unreachable states from the resulting DFA after determinization and before minimization.

The algorithm possesses shared memory in form of \(M\), the structure that maps a kept set of states to the number that it
is assigned on the final DFA, because the algorithm needs to check if a state has already been found after producing it.

#+CAPTION: Rabin Scott's Superset Construction Algorithm
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{SupersetConstruction}{\(M = (S, \Sigma, \delta, S_{0}, T)\)}
\State \(M\) \gets [(\(S_{0}, 0\))]
\State \(T^{'}\) \gets []
\If{\(\exists s \in S_{0}. s \in T\)}
    \State \(T^{'}\) \gets [\(S_{0}\)]
\EndIf
\State \(F\) \gets [\(S_{0}\)]
\While{\(F \neq \emptyset\)}
    \State \(S_{next}\) \gets pop from \(F\)
    \ForAll{\(a \in \Sigma\)}
        \State \(S^{'}\) \gets {}
        \ForAll{\(s \in S_{next}\)}
            \State Add \(s\) and all \epsilon transitions from \(s\)to \(S^{'}\)
        \EndFor
        \If{\(S_{next} \notin M\)}
            \State \(M\) \gets [\(M\), \((S^{'}, \left| M \right|)\)]
            \If{\(\exists s \in T. s \in S^{'}\)}
                \State \(T^{'}\) \gets [\(T^{'}, S^{'}\)]
            \EndIf
            \State \(F\) \gets [\(F\), \(S^{'}\)]
        \EndIf
        \State \(\delta^{'}\) \gets [\(\delta^{'}\), (\(S_{next}\), \(a\), \(S^{'}\))]
    \EndFor
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

Complexity-wise, the worst-case time complexity of the superset construction is \(O(2^{n})\), where \(n\) is the number of states in the original NFA.
Such worst-case is unavoidable as the size of the superset of states in the NFA \(\left| \mathds{S}(S) \right| = 2^{\left| S \right|}\), where \(S\) is the set of states in the original NFA.
However, this treshold is generally never reached, hence the purpose of the superset construction algorithm.

In terms of implementing the sequential version of the superset construction algorithm, most of the design decision comes in
how to store sets of states, as a state should be able to describe one of \(2^{n}\) possible states.

*** Storing Sets of States
The main challenge of superset construction implementation is not the implementation of the exploration algorithm, but
rather how to represent states of superset construction. The issues stems from how in superset construction, there
are about \(2^{n}\) possibly reachable states, so it is required to find a fast and memory-efficient way to store
such a state in a hash map. Furthermore, \cite{DBLP_journal} states that in a 2 billion-state DFA, each DFA state may
consist of upto 20 of the NFA states. Therefore, it is definitely required shorten the size of a state.

The solution implemented in the program is as such -
+ During superset construction, when a new state is being searched, represent the set of states as an array of bits.
  This representation is useful as bitwise operations can be done upon it, for a low cost.

+ Then, before hashing the set and inserting it to a hash map, compress the array. Here, the lz4 algorithm is used.
  The lz4 algorithm is a modern and fast byte array compression method that may simply return a byte array. It's main
  advantage is its speed compared to that of other compression algorithms, although it is not as size efficient.

+ The compressed array is inserted into the hash map. State storage size has been decreased for a moderate speed cost.


In =nfdeterminize=, this data structure is defined as a =Ubig= struct, which stands for =unsigned integer=. It is defined in
the =ubig.rs= source file.

** Approach to Minimization
While NFA determinization has been a well-known subject for a long time, DFA minimization on the other hand has less well-known algorithms. Out of all the minimization algorithms nowadays, 2 stand out as better
algorithms than the rest. Those are Hopcroft's algorithm and Brzozowski's minimization algorithm.

*** Hopcroft's Algorithm

Hopcroft's algorithm, made by J. Hopcroft in 1971\cite{Hopcroft1971AnNL},  is the first, and probably
the most well-known non-\(O(n^{2})\) time complexity DFA minimization algorithm. It is one of the first partition refinement algorithms.

Hopcrof't algorithm separates the states of the DFA into a partition of 2 sets - accept states and non-accept states. Those will be the states of the minimal automata by the end of the algorithm's execution.
Then, until the frontier is empty, it searches for states in the partition for which the transitions lead to distinguishable states.


If it is the case, then it means the partition has to be divided further. The algorithm is repeated until all states in each partition contain states that are indistinguishable by their transitions,
which means that the resulting DFA holds the same language than the original one, but at it's minimal size.

For definitions, let:
+ \(\mathcal{P}\): the partition to refine,
+ \(P \in \mathcal{P}\): a set of states in the partition.

Hopcroft's algorithm relies on the following lemma -
\newtheorem{lemma}{Lemma}
\begin{lemma}

Let some finite state machine \(M = (S, \Sigma, \delta, S_{p}, T)\).

\(\forall p \in S. \forall q \in S. \forall a \in \Sigma\), let \(\delta(p, a) = p^{'}\), \(\delta(q, a) = q^{'}\).

\(p^{'}\) and \(q^{'}\) are distinguishable \(\Rightarrow\) \(p\) and \(q\) are distinguishable.

\end{lemma}

Therefore, Hopcroft's algorithm uses the reverse transitions of the next set in the frontier to establish distinguishability between states in a set of the partition. Distinguishibility is therefore
defined as such, for some sets \(V, P \in \mathcal{P}\), and \(\delta^{-1}(P, a)\) the set of states \(s \in S s.t \delta(s, a) \in P\):

\[V \cap \delta^{-1}(P, a) \neq \emptyset \wedge V \backslash \delta^{-1}(P, a) \neq \emptyset \Rightarrow V \text{ is distinguishible into } V \cap \delta^{-1}(P, a) \text{ and } V \backslash \delta^{-1}(P, a)\]



#+CAPTION: Hopcroft's Algorithm
\begin{algorithm}
\label{algo:hopcroft}
\begin{algorithmic}[1]
\Procedure{HopcroftAlgo}{\(M = (S, \Sigma, \delta, s_{0}, T)\)}
    \State \(\mathcal{P}\) \gets \([T, S \backslash T]\)
    \State \(Q\) \gets \([T, S \backslash T]\)
    \While{\( \left| Q \right| \neq 0\)}
        \State \(P_{next}\) \gets pop \(Q\)
        \ForAll{a \in \(\Sigma\), V \in \(\mathcal{P}\)} \label{algo:hopcroft:forall}
            \If{\(\delta^{-1}(P_{next}, a) \cap V \neq \emptyset \cap V \backslash \delta^{-1}(P_{next}, a) \neq \emptyset\)}
                \State remove \(V\) from \(P\)
                \State push \(\delta^{-1}(P_{next}, a) \cap V\) into \(P\)
                \State push \(V \backslash \delta^{-1}(P_{next}, a)\) into \(P\)
                \If{\(V \in Q\)}
                    \State replace \([V]\) in \(Q\) with \([V \backslash \delta^{-1}(P_{next}, a), \delta^{-1}(P_{next}, a) \cap V]\)
                \ElsIf{\(\left| V \backslash \delta^{-1}(P_{next}, a) \right| \leq \left| \delta^{-1}(P_{next}, a) \cap V \right|\)}
                    \State add \(V \backslash \delta^{-1}(P_{next}, a)\) to \(Q\)
                \Else
                    \State add \(\delta^{-1}(P_{next}, a) \cap V\) to \(Q\)
                \EndIf
            \EndIf
        \EndFor
    \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

Hopcroft's Algorithm, as shown on figure \ref{algo:hopcroft}, has asymptotic time complexity \(O(knlog(n))\)\cite{Hopcroft1971AnNL}, where:
+ \(k\): the number of input letters in the alphabet \(\Sigma\),
+ \(n\): the number of states in the initial DFA.

Implementation-wise, the approach here is closer to the implementation described in \cite{Yingjie2009DescribingAN}, with some performance improvements.
On line \ref{algo:hopcroft:forall} of \ref{algo:hopcroft}, instead of looking for all \(V\) in \(\mathcal{P}\), it is possible to iterate through all partitions
linked to a state in \(\delta^{-1}(P_{next}, a)\), by keeping a map of what state is linked to which set in \(\mathcal{P}\). Doing so avoids the lengthy process of iterating
through \(\mathcal{P}\) for every set \(P_{next}\) in the frontier.

On the rust implementation, sets are represented as ordered vectors. With ordered vectors, difference and intersection construction can be done in \(O(n)\) time complexity, and
ordered vector construction from inverse transformation is done in \(O(n log(n))\) time complexity, for \(n\) the size of the set. Using a vector instead of a set avoids the overhead gotten from
consistently hashing values into a hash set.

Finally, the queue \(Q\) is done in a circular ring buffer as using contiguous memory, instead of a linked list, for faster memory access, while the partition is done as a simple contiguous memory array, as
it is never needed to pop anything from it. Instead, adding to the partition is done by replacing \(V\) by \(V \cap \delta^{-1}(P_{next}, a)\) and appending \(V \backslash \delta^{-1}(P_{next}, a)\) to the end of \(\mathcal{P}\).

*** Brzozowski's Algorithm

Brzozowski's algorithm is an exception to the general landscape of DFA minimization algorithms. Most minimization algorithms
work by doing partition refinement, like Hopcroft's, and some work by fusion like Revuz's \cite{REVUZ1992181}.
However, Brzozowski's algorithm works, for some finite state machine \(M = (S, \Sigma, \delta, S_{0}, T)\), by
determinizing \(M^{R} = (S, \Sigma, \delta^{-1}, T, S_{0})\), where \(\delta^{-1}\) is the table of inverse transitions from \(M\). Then, perform

determinization of \((M^{R})^{R}\). The result of the determinized \((M^{R})^{R}\) is the minimal DFA representation of \(M\).


This algorithm is very easy to implement as determinization has already been implemented beforehand. However, as with determinization,
it has an exponential time complexity.

Performance-wise however, Brzozowski's is known to outperform other minimization algorithm in particular cases, so it is interesting to support. Here, it
is supported via arguments to the =run= and =determinize= commands of =nfdeterminize=.

** Benchmarking
*** GAP-generation vs self-generation
#+TODO: Show benchmark, show why difference occurs
*** Pitfalls
#+TODO: epsilon transitions

* Multithreaded Approach
** Towards a Multithreaded Approach
#+TODO: Pitfalls of sequential approach
#+TODO: Issues of shared memory in determinization and minimization
** New Algorithms
*** Determinization
#+TODO: Pseudocode shared memory superset construction
*** Minimization
#+TODO: Pseudocode shared memory minimization algo
** Benchmarking
#+TODO: Results against sequential approach


* Appendix
** NFA to DFA patterns in Unit Tests
<<appendix:nfatodfa>>
#+CAPTION: Redundant NFA to redundant DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=3cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}


#+CAPTION: Empty language NFA to empty language DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \draw (s0) edge[above] node{a, b} (s1)
          (s1) edge[loop above] node{a, b} (s1);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}

#+CAPTION: Unreachable state in NFA removed in the DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=1.5cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=3cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial, accepting] (s0) {\(s_{0}\)};
    \draw (s0) edge[loop above] node{a, b} (s0);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}

#+CAPTION: NFA to DFA with sinkhole
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=1.7cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \node[state, accepting, right of=s1] (s2) {\(s_{2}\)};
    \draw (s0) edge[above] node{a} (s1)
        (s1) edge[above] node{a} (s2);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=1.7cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \node[state, below of=s1] (s2) {\(s_{2}\)};
    \node[state, accepting, right of=s1] (s3) {\(s_{3}\)};
    \draw (s0) edge[above] node{a} (s1)
        (s1) edge[above] node{a} (s3)
        (s0) edge[below] node{b} (s2)
        (s1) edge[right] node{b} (s2)
        (s3) edge[below] node{a, b} (s2)
        (s2) edge[loop below] node{a, b} (s2)
\end{tikzpicture}
\label{fig:redundant}
\end{figure}


#+CAPTION: NFA to DFA with sets of NFA states for states
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, accepting, right of=s0] (s1) {\(s_{1}\)};
    \draw (s0) edge[above] node{a} (s1)
        (s0) edge[loop above] node{a} (s0)
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, -0.5) {};
    \draw (0, 0) -> (1, 0);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\([s_{0}]\)};
    \node[state, accepting, right of=s0] (s1) {\([s_{0}, s_{1}]\)};
    \draw (s0) edge[above] node{a} (s1)
        (s1) edge[loop above] node{a} (s1)
\end{tikzpicture}
\label{fig:redundant}
\end{figure}



#+CAPTION: \epsilon automaton to DFA
\begin{figure}
\centering
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\(s_{0}\)};
    \node[state, right of=s0] (s1) {\(s_{1}\)};
    \node[state, below of=s0] (s2) {\(s_{2}\)};
    \node[state, accepting, below of=s1] (s3) {\(s_{3}\)};
    \draw (s0) edge[above] node{\(\epsilon\)} (s1)
        (s0) edge[left] node{a} (s2)
        (s1) edge[right] node{a} (s3)
        (s2) edge[below] node{b} (s3)
        (s3) edge[loop below] node{a, b} (s3);
\end{tikzpicture}
\begin{tikzpicture}[->]
    \node (ph) at (0.5, 0) {};
    \draw (0, 2) -> (1, 2);
\end{tikzpicture}
\begin{tikzpicture}[->, >=stealth, node distance=2cm, every state/.style={thick, fill=gray!10}, initial text=$ $]
    \node[state, initial] (s0) {\([s_{0}]\)};
    \node[state, accepting, right of=s0] (s1) {\([s_{1}, s_{3}]\)};
    \node[state, below of=s0] (s2) {\([]\)};
    \node[state, accepting, below of=s1] (s3) {\([s_{3}]\)};
    \draw (s0) edge[above] node{\(\epsilon\)} (s1)
        (s0) edge[left] node{a} (s2)
        (s1) edge[right] node{a} (s3)
        (s2) edge[loop below] node{a, b} (s2)
        (s3) edge[loop below] node{a, b} (s3);
\end{tikzpicture}
\label{fig:redundant}
\end{figure}


\bibliographystyle{ieeetr}
\bibliography{dissertation}
